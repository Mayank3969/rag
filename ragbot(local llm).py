import os
from langchain_community.document_loaders import UnstructuredPDFLoader, UnstructuredPowerPointLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain.chains import RetrievalQA

#1 Load all documents
def load_docs(folder_path):
    
    docs = []
    for filename in os.listdir(folder_path):
        full_path = os.path.join(folder_path, filename)
        if filename.endswith(".pdf"):
            print(f"ğŸ“„ Loading PDF: {filename}")
            loader = UnstructuredPDFLoader(full_path)
        elif filename.endswith(".pptx"):
            print(f"ğŸ“Š Loading PPTX: {filename}")
            loader = UnstructuredPowerPointLoader(full_path)
        else:
            continue
        docs.extend(loader.load())
    return docs

#Split docs
def split_docs(docs):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    return splitter.split_documents(docs)

#Create or load vector store
def create_or_load_vectorstore(chunks):
    embeddings = OllamaEmbeddings(model="mxbai-embed-large")
    db = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory="chroma_db")
    return db

#Create the QA chain with Retriever and Dolphin-Mistral
def create_qa_chain(vectorstore):
    retriever = vectorstore.as_retriever()
    llm = OllamaLLM(model="dolphin-mistral:7b")
    qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=False)
    return qa

#Main Interactive Loop
def main():
    folder_path = "docs"
    print("ğŸ“‚ Loading documents...")
    docs = load_docs(folder_path)
    print(f"âœ… Loaded {len(docs)} documents.")

    print("âœ‚ï¸ Splitting into chunks...")
    chunks = split_docs(docs)

    print("ğŸ” Creating vector store...")
    vectorstore = create_or_load_vectorstore(chunks)

    print("ğŸ¤– Starting chatbot with Dolphin-Mistral...")
    qa_chain = create_qa_chain(vectorstore)

    while True:
        question = input("\nâ“ Ask your question (or type 'exit'): ")
        if question.lower() == 'exit':
            break
        try:
            response = qa_chain.invoke({"query": question})
            print("ğŸ’¬ Answer:", response['result'])
        except Exception as e:
            print("âš ï¸ Error:", e)

if __name__ == "__main__":
    main()